import os
import json
import torch
import argparse
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import numpy as np

class SentimentAnalyzer:
    def __init__(self, model_path="sentiment_model/final_model"):
        """
        Initialize the sentiment analyzer with a trained model
        
        Args:
            model_path (str): Path to the directory containing the model and tokenizer
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")
        
        # Load model and tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)
        
        # Load label mapping
        try:
            mapping_path = os.path.join(os.path.dirname(model_path), "label_mapping.json")
            with open(mapping_path, "r") as f:
                mapping = json.load(f)
                self.id2label = mapping["id2label"]
                # Convert string keys back to integers
                self.id2label = {int(k): v for k, v in self.id2label.items()}
        except FileNotFoundError:
            print(f"Warning: Label mapping file not found at {mapping_path}")
            print("Using default mapping: {0: 'negative', 1: 'positive'}")
            self.id2label = {0: 'negative', 1: 'positive'}
        
        # Create a pipeline for easier inference
        self.pipeline = pipeline(
            "text-classification", 
            model=self.model, 
            tokenizer=self.tokenizer,
            device=0 if self.device == "cuda" else -1
        )
        
        # Load training metadata if available
        try:
            metadata_path = os.path.join(os.path.dirname(model_path), "training_metadata.json")
            with open(metadata_path, "r") as f:
                self.metadata = json.load(f)
                print(f"Model trained with {self.metadata['accuracy']:.4f} accuracy on test set")
        except FileNotFoundError:
            self.metadata = None
            print("No training metadata found")
    
    def predict(self, text, return_probabilities=False):
        """
        Predict sentiment for a single text
        
        Args:
            text (str): The input text
            return_probabilities (bool): Whether to return probability scores
            
        Returns:
            dict: Prediction result with sentiment and confidence
        """
        # Handle empty or invalid input
        if not isinstance(text, str) or not text.strip():
            return {"sentiment": "neutral", "confidence": 0.0}
        
        # Tokenize input
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True).to(self.device)
        
        # Get prediction
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            
            # Get predicted class and confidence
            pred_class = torch.argmax(probs, dim=1).item()
            confidence = probs[0, pred_class].item()
            
            result = {
                "sentiment": self.id2label[pred_class],
                "confidence": confidence
            }
            
            # Add probabilities if requested
            if return_probabilities:
                probs_dict = {self.id2label[i]: float(probs[0, i].item()) 
                             for i in range(len(self.id2label))}
                result["probabilities"] = probs_dict
                
            return result
    
    def predict_batch(self, texts, batch_size=8):
        """
        Predict sentiment for a batch of texts
        
        Args:
            texts (list): List of input texts
            batch_size (int): Batch size for prediction
            
        Returns:
            list: List of prediction results
        """
        results = self.pipeline(texts, batch_size=batch_size)
        
        # Convert pipeline output format to our format
        formatted_results = []
        for result in results:
            label = result["label"]
            score = result["score"]
            formatted_results.append({"sentiment": label, "confidence": score})
            
        return formatted_results
    
    def analyze_text_details(self, text):
        """
        Provide a detailed analysis of the sentiment in the text
        
        Args:
            text (str): The input text
            
        Returns:
            dict: Detailed sentiment analysis
        """
        basic_result = self.predict(text, return_probabilities=True)
        
        # Get confidence difference between positive and negative
        probs = basic_result.get("probabilities", {})
        pos_conf = probs.get("positive", 0)
        neg_conf = probs.get("negative", 0)
        confidence_diff = abs(pos_conf - neg_conf)
        
        # Determine sentiment strength
        if confidence_diff < 0.2:
            strength = "weak"
        elif confidence_diff < 0.6:
            strength = "moderate"
        else:
            strength = "strong"
            
        # Create detailed output
        detailed = {
            "sentiment": basic_result["sentiment"],
            "confidence": basic_result["confidence"],
            "sentiment_strength": strength,
            "probabilities": basic_result.get("probabilities", {}),
            "text_length": len(text.split())
        }
        
        return detailed

def interactive_mode(analyzer):
    """Run the sentiment analyzer in interactive mode"""
    print("\n===== Sentiment Analysis Interactive Mode =====")
    print("Enter 'quit', 'exit', or 'q' to exit")
    print("Enter 'detailed' before your text for a detailed analysis")
    
    while True:
        user_input = input("\nEnter text to analyze: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("Exiting interactive mode.")
            break
            
        # Check if detailed analysis is requested
        detailed_mode = False
        if user_input.lower().startswith('detailed '):
            detailed_mode = True
            user_input = user_input[9:].strip()  # Remove 'detailed ' prefix
            
        if not user_input:
            print("Please enter some text to analyze.")
            continue
            
        try:
            if detailed_mode:
                result = analyzer.analyze_text_details(user_input)
                print(f"\nSentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})")
                print(f"Strength: {result['sentiment_strength']}")
                print(f"Word count: {result['text_length']}")
                print("Probabilities:")
                for label, prob in result['probabilities'].items():
                    print(f"  {label}: {prob:.4f}")
            else:
                result = analyzer.predict(user_input)
                print(f"\nSentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})")
        except Exception as e:
            print(f"Error analyzing text: {e}")

def main():
    parser = argparse.ArgumentParser(description="Sentiment analysis using a fine-tuned model")
    parser.add_argument("--model_path", type=str, default="sentiment_model/final_model",
                       help="Path to the fine-tuned model directory")
    parser.add_argument("--interactive", action="store_true",
                       help="Run in interactive mode")
    parser.add_argument("--text", type=str, default=None,
                       help="Text to analyze (when not in interactive mode)")
    parser.add_argument("--detailed", action="store_true",
                       help="Provide detailed analysis")
    parser.add_argument("--batch_file", type=str, default=None,
                       help="Path to a file with one text per line for batch processing")
    parser.add_argument("--output_file", type=str, default=None,
                       help="Path to save batch processing results (JSON format)")
    
    args = parser.parse_args()
    
    # Initialize the analyzer
    analyzer = SentimentAnalyzer(model_path=args.model_path)
    
    # Run in different modes based on arguments
    if args.interactive:
        interactive_mode(analyzer)
        
    elif args.batch_file:
        # Process batch of texts from file
        try:
            with open(args.batch_file, 'r', encoding='utf-8') as f:
                texts = [line.strip() for line in f if line.strip()]
                
            print(f"Processing {len(texts)} texts from {args.batch_file}...")
            results = analyzer.predict_batch(texts)
            
            if args.output_file:
                with open(args.output_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2)
                print(f"Results saved to {args.output_file}")
            else:
                # Print first 5 results
                for i, (text, result) in enumerate(zip(texts[:5], results[:5])):
                    print(f"{i+1}. Text: {text[:50]}{'...' if len(text) > 50 else ''}")
                    print(f"   Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})")
                
                if len(texts) > 5:
                    print(f"... and {len(texts)-5} more results")
        except Exception as e:
            print(f"Error processing batch file: {e}")
            
    elif args.text:
        # Process a single text
        try:
            if args.detailed:
                result = analyzer.analyze_text_details(args.text)
                print(f"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})")
                print(f"Strength: {result['sentiment_strength']}")
                print(f"Word count: {result['text_length']}")
                print("Probabilities:")
                for label, prob in result['probabilities'].items():
                    print(f"  {label}: {prob:.4f}")
            else:
                result = analyzer.predict(args.text)
                print(f"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})")
        except Exception as e:
            print(f"Error analyzing text: {e}")
            
    else:
        # No specific mode selected, show some examples
        example_texts = [
            "I absolutely loved this movie! The acting was superb.",
            "This product is terrible. It broke after one use.",
            "The restaurant was okay. Food was good but service was slow.",
            "I don't have strong feelings about this book either way."
        ]
        
        print("\n===== Sentiment Analysis Examples =====")
        for text in example_texts:
            result = analyzer.predict(text)
            print(f"\nText: {text}")
            print(f"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.2f})")
            
        print("\nRun with --interactive flag for interactive mode")
        print("Run with --help for more options")

if __name__ == "__main__":
    main()